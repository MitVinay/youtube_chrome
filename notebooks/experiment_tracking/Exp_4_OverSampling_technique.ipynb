{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MKtExwgzv720"
      },
      "outputs": [],
      "source": [
        "# ! pip install nltk textacy\n",
        "# ! pip install scikit-learn\n",
        "# ! pip install mlflow\n",
        "# ! pip install dagshub\n",
        "# ! pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from textacy import preprocessing\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import mlflow\n",
        "import dagshub\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE, KMeansSMOTE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQfmlylIwGwM",
        "outputId": "d31eac37-c746-41d3-c82a-6afaff7004ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processing():\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/Himanshu-1703/reddit-sentiment-analysis/refs/heads/main/data/reddit.csv')\n",
        "  print(\"Shape of the data frame\", df.shape)\n",
        "  print(\"Duplicates\", df.duplicated().sum())\n",
        "  print(\"Null Values:\" ,df.isnull().sum())\n",
        "\n",
        "  print(\"Dropping the duplicate records.....\")\n",
        "  df.drop_duplicates(inplace=True)\n",
        "\n",
        "  print(\"Dropping the null values\")\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  print(\"Changing data to lower case\")\n",
        "  df['clean_comment'] = df['clean_comment'].str.lower()\n",
        "\n",
        "  df['length_clean_comment'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "\n",
        "  print(\"Strip off the white spaces..\")\n",
        "  df['clean_comment'] = df['clean_comment'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "  df['length_clean_comment_nowhite_space'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have white spaces:\" , df[df['length_clean_comment']!= df['length_clean_comment_nowhite_space']].shape[0])\n",
        "\n",
        "  print(\"Removing Html tags....\")\n",
        "  df['clean_comment'] = df['clean_comment'].apply(preprocessing.remove.html_tags)\n",
        "  df['length_nowhite_space_htmltag'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have Html tags:\" , df[df['length_clean_comment_nowhite_space']!= df['length_nowhite_space_htmltag']].shape[0])\n",
        "\n",
        "  print(\"Removing Punctuation....\")\n",
        "  df['clean_comment'] = df['clean_comment'].apply(preprocessing.remove.punctuation)\n",
        "  df['length_htmltag_punctuation'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have punctuation:\" , df[df['length_nowhite_space_htmltag']!= df['length_htmltag_punctuation']].shape[0])\n",
        "\n",
        "  print(\"Removing brackets....\")\n",
        "  df['clean_comment'] = df['clean_comment'].apply(preprocessing.remove.punctuation)\n",
        "  df['length_punctuation_brackets'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have brackets:\" , df[df['length_htmltag_punctuation']!= df['length_punctuation_brackets']].shape[0])\n",
        "\n",
        "  # Apply the function to the 'clean_comment' column in a single line\n",
        "  df['clean_comment'] = df['clean_comment'].apply(lambda x: preprocessing.replace.emojis(x, \"\"))\n",
        "  df['length_brackets_emojis'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have emojis:\" , df[df['length_punctuation_brackets']!= df['length_brackets_emojis']].shape[0])\n",
        "\n",
        "\n",
        "  # Regular expression to match emojis\n",
        "  emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\"  # Emoticons\n",
        "                            \"\\U0001F300-\\U0001F5FF\"  # Symbols and Pictographs\n",
        "                            \"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
        "                            \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
        "                            \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                            \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                            \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                            \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                            \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                            \"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "                            \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
        "                            \"]\", flags=re.UNICODE)\n",
        "\n",
        "  # Filter out rows where 'clean_comment' contains emojis\n",
        "  print(\"Number of non meaning  rows:\", df[df['clean_comment'].apply(lambda x: bool(emoji_pattern.search(x)))].shape)\n",
        "  df = df[~df['clean_comment'].apply(lambda x: bool(emoji_pattern.search(x)))]\n",
        "\n",
        "\n",
        "  # List of words to keep even if their length is less than 4\n",
        "  keep_words = ['lol', 'wow', 'wtf', 'fun', 'sad', 'old']\n",
        "\n",
        "  # Filter out rows with clean_comment length < 4 unless they contain one of the keep_words\n",
        "  df = df[(df['length_clean_comment'] >= 4) | df['clean_comment'].isin(keep_words)]\n",
        "\n",
        "  return df[['clean_comment', 'category', 'length_clean_comment']]\n",
        "\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_comment(comment):\n",
        "    # Convert to lowercase\n",
        "    comment = comment.lower()\n",
        "\n",
        "    # Remove trailing and leading whitespaces\n",
        "    comment = comment.strip()\n",
        "\n",
        "    # Remove newline characters\n",
        "    comment = re.sub(r'\\n', ' ', comment)\n",
        "\n",
        "    # Remove non-alphanumeric characters, except punctuation\n",
        "    comment = re.sub(r'[^A-Za-z0-9\\s!?.,]', '', comment)\n",
        "\n",
        "    # Remove stopwords but retain important ones for sentiment analysis\n",
        "    stop_words = set(stopwords.words('english')) - {'not', 'but', 'however', 'no', 'yet'}\n",
        "    comment = ' '.join([word for word in comment.split() if word not in stop_words])\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    comment = ' '.join([lemmatizer.lemmatize(word) for word in comment.split()])\n",
        "\n",
        "    return comment\n",
        "\n",
        "df = pre_processing()\n",
        "df['clean_comment'] = df['clean_comment'].apply(preprocess_comment)\n",
        "# Assuming df is already loaded\n",
        "X = df['clean_comment']\n",
        "y = df['category']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "# Initialize the vectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,1), max_features=5000)\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMGWRqHOwKEL",
        "outputId": "f31620a5-6ad5-4f1a-f009-ed728b55d0a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the data frame (37249, 2)\n",
            "Duplicates 449\n",
            "Null Values: clean_comment    100\n",
            "category           0\n",
            "dtype: int64\n",
            "Dropping the duplicate records.....\n",
            "Dropping the null values\n",
            "Changing data to lower case\n",
            "Strip off the white spaces..\n",
            "Number of rows have white spaces: 32407\n",
            "Removing Html tags....\n",
            "Number of rows have Html tags: 0\n",
            "Removing Punctuation....\n",
            "Number of rows have punctuation: 0\n",
            "Removing brackets....\n",
            "Number of rows have brackets: 0\n",
            "Number of rows have emojis: 55\n",
            "Number of non meaning  rows: (148, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# oversampling_algo = {\"SMOTE\": SMOTE(), \"ADASYN\": ADASYN(),\"BorderlineSMOTE\": BorderlineSMOTE(),\n",
        "#                      \"KMeansSMOTE\": KMeansSMOTE(),\"SVMSMOTE\": SVMSMOTE()}\n",
        "\n",
        "oversampling_algo = {\"SVMSMOTE\": SVMSMOTE()}\n",
        ""
      ],
      "metadata": {
        "id": "Xagt5lBdwZ9O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dagshub.init(repo_owner='MitVinay', repo_name='youtube_chrome', mlflow=True)\n",
        "mlflow.set_experiment(\"Exp3-oversampling Technique\")\n",
        "\n",
        "# Start parent run\n",
        "with mlflow.start_run() as parent_run:\n",
        "    # Define the n-grams range and max_features values\n",
        "\n",
        "    # Automate the process for TfidfVectorizer\n",
        "    for oversampling, algo in oversampling_algo.items():\n",
        "        with mlflow.start_run(nested=True, run_name=f\"{oversampling} TFIDF , 1gram max_features=5000\") as child_run:\n",
        "            print(f\"oversampling: {oversampling}\")\n",
        "\n",
        "            X_resampled, y_resampled = algo.fit_resample(X_train, y_train)\n",
        "            print(sorted(Counter(y_resampled).items()))\n",
        "            # Train the RandomForestClassifier\n",
        "            rf = RandomForestClassifier(random_state=42)\n",
        "            rf.fit(X_resampled, y_resampled)\n",
        "\n",
        "            # Predict and evaluate\n",
        "            y_pred = rf.predict(X_test)\n",
        "            metrics = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "            for label, metrics_dict in metrics.items():\n",
        "                if label != 'accuracy':  # 'accuracy' is logged separately as a single value\n",
        "                    for metric, value in metrics_dict.items():\n",
        "                        mlflow.log_metric(f\"{label}_{metric}\", value)\n",
        "                else:\n",
        "                    # Log the accuracy score separately\n",
        "                    mlflow.log_metric(\"accuracy\", metrics_dict)\n",
        "\n",
        "            mlflow.log_param(\"max_features\", 5000)\n",
        "            mlflow.log_param(\"ngram_range\", 1)\n",
        "            mlflow.log_param(\"vectorizer\", \"TfidfVectorizer\")\n",
        "            mlflow.log_param(\"Under Sampling Technique\", oversampling)\n",
        "            mlflow.sklearn.log_model(rf, \"model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "orbkJQq5yYkS",
        "outputId": "56a930ea-44fa-4aad-a65f-29e36e0f9865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"MitVinay/youtube_chrome\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"MitVinay/youtube_chrome\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository MitVinay/youtube_chrome initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository MitVinay/youtube_chrome initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oversampling: SVMSMOTE\n"
          ]
        }
      ]
    }
  ]
}
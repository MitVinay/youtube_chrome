{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4e98d014720f4a05a20f5016faeaf688": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f30d7c61019e4c959589b0dbdb2525d3",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m‚†π\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚†π</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "f30d7c61019e4c959589b0dbdb2525d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ! pip install nltk textacy\n",
        "# ! pip install scikit-learn\n",
        "# ! pip install mlflow\n",
        "# ! pip install dagshub\n",
        "# ! pip install imbalanced-learn"
      ],
      "metadata": {
        "id": "cRxXJssl0L2J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN7NJR3WzoKV",
        "outputId": "997939b1-4b56-42c8-ffed-c508f62ba904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from textacy import preprocessing\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import mlflow\n",
        "import dagshub\n",
        "from collections import Counter\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processing():\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/Himanshu-1703/reddit-sentiment-analysis/refs/heads/main/data/reddit.csv')\n",
        "  print(\"Shape of the data frame\", df.shape)\n",
        "  print(\"Duplicates\", df.duplicated().sum())\n",
        "  print(\"Null Values:\" ,df.isnull().sum())\n",
        "\n",
        "  print(\"Dropping the duplicate records.....\")\n",
        "  df.drop_duplicates(inplace=True)\n",
        "\n",
        "  print(\"Dropping the null values\")\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  print(\"Changing data to lower case\")\n",
        "  df['clean_comment'] = df['clean_comment'].str.lower()\n",
        "\n",
        "  df['length_clean_comment'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "\n",
        "  print(\"Strip off the white spaces..\")\n",
        "  df['clean_comment'] = df['clean_comment'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "  df['length_clean_comment_nowhite_space'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have white spaces:\" , df[df['length_clean_comment']!= df['length_clean_comment_nowhite_space']].shape[0])\n",
        "\n",
        "  print(\"Removing Html tags....\")\n",
        "  df['clean_comment'] = df['clean_comment'].apply(preprocessing.remove.html_tags)\n",
        "  df['length_nowhite_space_htmltag'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have Html tags:\" , df[df['length_clean_comment_nowhite_space']!= df['length_nowhite_space_htmltag']].shape[0])\n",
        "\n",
        "  print(\"Removing Punctuation....\")\n",
        "  df['clean_comment'] = df['clean_comment'].apply(preprocessing.remove.punctuation)\n",
        "  df['length_htmltag_punctuation'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have punctuation:\" , df[df['length_nowhite_space_htmltag']!= df['length_htmltag_punctuation']].shape[0])\n",
        "\n",
        "  print(\"Removing brackets....\")\n",
        "  df['clean_comment'] = df['clean_comment'].apply(preprocessing.remove.punctuation)\n",
        "  df['length_punctuation_brackets'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have brackets:\" , df[df['length_htmltag_punctuation']!= df['length_punctuation_brackets']].shape[0])\n",
        "\n",
        "  # Apply the function to the 'clean_comment' column in a single line\n",
        "  df['clean_comment'] = df['clean_comment'].apply(lambda x: preprocessing.replace.emojis(x, \"\"))\n",
        "  df['length_brackets_emojis'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have emojis:\" , df[df['length_punctuation_brackets']!= df['length_brackets_emojis']].shape[0])\n",
        "\n",
        "\n",
        "  # Regular expression to match emojis\n",
        "  emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\"  # Emoticons\n",
        "                            \"\\U0001F300-\\U0001F5FF\"  # Symbols and Pictographs\n",
        "                            \"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
        "                            \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
        "                            \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                            \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                            \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                            \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                            \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                            \"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "                            \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
        "                            \"]\", flags=re.UNICODE)\n",
        "\n",
        "  # Filter out rows where 'clean_comment' contains emojis\n",
        "  print(\"Number of non meaning  rows:\", df[df['clean_comment'].apply(lambda x: bool(emoji_pattern.search(x)))].shape)\n",
        "  df = df[~df['clean_comment'].apply(lambda x: bool(emoji_pattern.search(x)))]\n",
        "\n",
        "\n",
        "  # List of words to keep even if their length is less than 4\n",
        "  keep_words = ['lol', 'wow', 'wtf', 'fun', 'sad', 'old']\n",
        "\n",
        "  # Filter out rows with clean_comment length < 4 unless they contain one of the keep_words\n",
        "  df = df[(df['length_clean_comment'] >= 4) | df['clean_comment'].isin(keep_words)]\n",
        "\n",
        "  return df[['clean_comment', 'category', 'length_clean_comment']]\n",
        "\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_comment(comment):\n",
        "    # Convert to lowercase\n",
        "    comment = comment.lower()\n",
        "\n",
        "    # Remove trailing and leading whitespaces\n",
        "    comment = comment.strip()\n",
        "\n",
        "    # Remove newline characters\n",
        "    comment = re.sub(r'\\n', ' ', comment)\n",
        "\n",
        "    # Remove non-alphanumeric characters, except punctuation\n",
        "    comment = re.sub(r'[^A-Za-z0-9\\s!?.,]', '', comment)\n",
        "\n",
        "    # Remove stopwords but retain important ones for sentiment analysis\n",
        "    stop_words = set(stopwords.words('english')) - {'not', 'but', 'however', 'no', 'yet'}\n",
        "    comment = ' '.join([word for word in comment.split() if word not in stop_words])\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    comment = ' '.join([lemmatizer.lemmatize(word) for word in comment.split()])\n",
        "\n",
        "    return comment\n",
        "\n",
        "df = pre_processing()\n",
        "df['clean_comment'] = df['clean_comment'].apply(preprocess_comment)\n",
        "# Assuming df is already loaded\n",
        "X = df['clean_comment']\n",
        "y = df['category']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "# Initialize the vectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,1), max_features=5000)\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCgEUEv80tKF",
        "outputId": "fabb2fd7-035b-4b45-f3a3-bcfa5d9dcc09"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the data frame (37249, 2)\n",
            "Duplicates 449\n",
            "Null Values: clean_comment    100\n",
            "category           0\n",
            "dtype: int64\n",
            "Dropping the duplicate records.....\n",
            "Dropping the null values\n",
            "Changing data to lower case\n",
            "Strip off the white spaces..\n",
            "Number of rows have white spaces: 32407\n",
            "Removing Html tags....\n",
            "Number of rows have Html tags: 0\n",
            "Removing Punctuation....\n",
            "Number of rows have punctuation: 0\n",
            "Removing brackets....\n",
            "Number of rows have brackets: 0\n",
            "Number of rows have emojis: 55\n",
            "Number of non meaning  rows: (148, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oversampling_algo = {\"SMOTETomek\": SMOTETomek(), \"SMOTEENN\": SMOTEENN()}"
      ],
      "metadata": {
        "id": "Ug5q_frY0xn9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dagshub.init(repo_owner='MitVinay', repo_name='youtube_chrome', mlflow=True)\n",
        "mlflow.set_experiment(\"Exp3-Combined Technique\")\n",
        "\n",
        "# Start parent run\n",
        "with mlflow.start_run() as parent_run:\n",
        "    # Define the n-grams range and max_features values\n",
        "\n",
        "    # Automate the process for TfidfVectorizer\n",
        "    for oversampling, algo in oversampling_algo.items():\n",
        "        with mlflow.start_run(nested=True, run_name=f\"{oversampling} TFIDF , 1gram max_features=5000\") as child_run:\n",
        "            print(f\"oversampling: {oversampling}\")\n",
        "\n",
        "            X_resampled, y_resampled = algo.fit_resample(X_train, y_train)\n",
        "            print(sorted(Counter(y_resampled).items()))\n",
        "            # Train the RandomForestClassifier\n",
        "            rf = RandomForestClassifier(random_state=42)\n",
        "            rf.fit(X_resampled, y_resampled)\n",
        "\n",
        "            # Predict and evaluate\n",
        "            y_pred = rf.predict(X_test)\n",
        "            metrics = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "            for label, metrics_dict in metrics.items():\n",
        "                if label != 'accuracy':  # 'accuracy' is logged separately as a single value\n",
        "                    for metric, value in metrics_dict.items():\n",
        "                        mlflow.log_metric(f\"{label}_{metric}\", value)\n",
        "                else:\n",
        "                    # Log the accuracy score separately\n",
        "                    mlflow.log_metric(\"accuracy\", metrics_dict)\n",
        "\n",
        "            mlflow.log_param(\"max_features\", 5000)\n",
        "            mlflow.log_param(\"ngram_range\", 1)\n",
        "            mlflow.log_param(\"vectorizer\", \"TfidfVectorizer\")\n",
        "            mlflow.log_param(\"Combined Technique\", oversampling)\n",
        "            mlflow.sklearn.log_model(rf, \"model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431,
          "referenced_widgets": [
            "4e98d014720f4a05a20f5016faeaf688",
            "f30d7c61019e4c959589b0dbdb2525d3"
          ]
        },
        "id": "ejTvTwpq08sY",
        "outputId": "7e87482a-0702-42a3-a0e3-4795fc0a0d37"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1m‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=02972bb2-b1e4-4321-8dfe-82bbe9ad3d03&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=021ac6a3aeac52024e40527d59280de21e8f69de8b826536071d5470e3464ef5\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e98d014720f4a05a20f5016faeaf688"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as MitVinay\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as MitVinay\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"MitVinay/youtube_chrome\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"MitVinay/youtube_chrome\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository MitVinay/youtube_chrome initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository MitVinay/youtube_chrome initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/12/13 16:25:37 INFO mlflow.tracking.fluent: Experiment with name 'Exp3-Combined Technique' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oversampling: SMOTETomek\n",
            "[(-1, 10945), (0, 10773), (1, 10786)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2024/12/13 16:27:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèÉ View run SMOTETomek TFIDF , 1gram max_features=5000 at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/6/runs/ae6a1e7ddb7346d080a1f0f16d3a44c5\n",
            "üß™ View experiment at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/6\n",
            "oversampling: SMOTEENN\n",
            "[(-1, 6893), (0, 4707), (1, 628)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2024/12/13 16:29:10 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèÉ View run SMOTEENN TFIDF , 1gram max_features=5000 at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/6/runs/ab4c7fedb9e447438be6c334820bfa32\n",
            "üß™ View experiment at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/6\n",
            "üèÉ View run zealous-duck-856 at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/6/runs/77bf63a563fc480c83e2c8b972120c9b\n",
            "üß™ View experiment at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/6\n"
          ]
        }
      ]
    }
  ]
}
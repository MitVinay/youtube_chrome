{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb47cdcdad974df1945a34f2b20d6504": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_7451a201701a47ebb817a004a7fd469f",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32mтаз\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">таз</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "7451a201701a47ebb817a004a7fd469f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ! pip install nltk textacy\n",
        "# ! pip install scikit-learn\n",
        "# ! pip install mlflow\n",
        "# ! pip install dagshub"
      ],
      "metadata": {
        "id": "erv5IAVyf4SC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from textacy import preprocessing\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import mlflow\n",
        "import dagshub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTP0-S70fJGk",
        "outputId": "bd4a81ce-9878-4e0f-91ca-2f4b30d5159b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0voNIAkAe2Pr"
      },
      "outputs": [],
      "source": [
        "def pre_processing():\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/Himanshu-1703/reddit-sentiment-analysis/refs/heads/main/data/reddit.csv')\n",
        "  print(\"Shape of the data frame\", df.shape)\n",
        "  print(\"Duplicates\", df.duplicated().sum())\n",
        "  print(\"Null Values:\" ,df.isnull().sum())\n",
        "\n",
        "  print(\"Dropping the duplicate records.....\")\n",
        "  df.drop_duplicates(inplace=True)\n",
        "\n",
        "  print(\"Dropping the null values\")\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  print(\"Changing data to lower case\")\n",
        "  df['clean_comment'] = df['clean_comment'].str.lower()\n",
        "\n",
        "  df['length_clean_comment'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "\n",
        "  print(\"Strip off the white spaces..\")\n",
        "  df['clean_comment'] = df['clean_comment'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "  df['length_clean_comment_nowhite_space'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have white spaces:\" , df[df['length_clean_comment']!= df['length_clean_comment_nowhite_space']].shape[0])\n",
        "\n",
        "  print(\"Removing Html tags....\")\n",
        "  df['clean_comment'] = df['clean_comment'].apply(preprocessing.remove.html_tags)\n",
        "  df['length_nowhite_space_htmltag'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have Html tags:\" , df[df['length_clean_comment_nowhite_space']!= df['length_nowhite_space_htmltag']].shape[0])\n",
        "\n",
        "  print(\"Removing Punctuation....\")\n",
        "  df['clean_comment'] = df['clean_comment'].apply(preprocessing.remove.punctuation)\n",
        "  df['length_htmltag_punctuation'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have punctuation:\" , df[df['length_nowhite_space_htmltag']!= df['length_htmltag_punctuation']].shape[0])\n",
        "\n",
        "  print(\"Removing brackets....\")\n",
        "  df['clean_comment'] = df['clean_comment'].apply(preprocessing.remove.punctuation)\n",
        "  df['length_punctuation_brackets'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have brackets:\" , df[df['length_htmltag_punctuation']!= df['length_punctuation_brackets']].shape[0])\n",
        "\n",
        "  # Apply the function to the 'clean_comment' column in a single line\n",
        "  df['clean_comment'] = df['clean_comment'].apply(lambda x: preprocessing.replace.emojis(x, \"\"))\n",
        "  df['length_brackets_emojis'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have emojis:\" , df[df['length_punctuation_brackets']!= df['length_brackets_emojis']].shape[0])\n",
        "\n",
        "\n",
        "  # Regular expression to match emojis\n",
        "  emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\"  # Emoticons\n",
        "                            \"\\U0001F300-\\U0001F5FF\"  # Symbols and Pictographs\n",
        "                            \"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
        "                            \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
        "                            \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                            \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                            \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                            \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                            \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                            \"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "                            \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
        "                            \"]\", flags=re.UNICODE)\n",
        "\n",
        "  # Filter out rows where 'clean_comment' contains emojis\n",
        "  print(\"Number of non meaning  rows:\", df[df['clean_comment'].apply(lambda x: bool(emoji_pattern.search(x)))].shape)\n",
        "  df = df[~df['clean_comment'].apply(lambda x: bool(emoji_pattern.search(x)))]\n",
        "\n",
        "\n",
        "  # List of words to keep even if their length is less than 4\n",
        "  keep_words = ['lol', 'wow', 'wtf', 'fun', 'sad', 'old']\n",
        "\n",
        "  # Filter out rows with clean_comment length < 4 unless they contain one of the keep_words\n",
        "  df = df[(df['length_clean_comment'] >= 4) | df['clean_comment'].isin(keep_words)]\n",
        "\n",
        "  return df[['clean_comment', 'category', 'length_clean_comment']]\n",
        "\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_comment(comment):\n",
        "    # Convert to lowercase\n",
        "    comment = comment.lower()\n",
        "\n",
        "    # Remove trailing and leading whitespaces\n",
        "    comment = comment.strip()\n",
        "\n",
        "    # Remove newline characters\n",
        "    comment = re.sub(r'\\n', ' ', comment)\n",
        "\n",
        "    # Remove non-alphanumeric characters, except punctuation\n",
        "    comment = re.sub(r'[^A-Za-z0-9\\s!?.,]', '', comment)\n",
        "\n",
        "    # Remove stopwords but retain important ones for sentiment analysis\n",
        "    stop_words = set(stopwords.words('english')) - {'not', 'but', 'however', 'no', 'yet'}\n",
        "    comment = ' '.join([word for word in comment.split() if word not in stop_words])\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    comment = ' '.join([lemmatizer.lemmatize(word) for word in comment.split()])\n",
        "\n",
        "    return comment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pre_processing()\n",
        "df['clean_comment'] = df['clean_comment'].apply(preprocess_comment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EARrrIfKfIZI",
        "outputId": "bda98f42-7286-4d4d-a198-34e1013040d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the data frame (37249, 2)\n",
            "Duplicates 449\n",
            "Null Values: clean_comment    100\n",
            "category           0\n",
            "dtype: int64\n",
            "Dropping the duplicate records.....\n",
            "Dropping the null values\n",
            "Changing data to lower case\n",
            "Strip off the white spaces..\n",
            "Number of rows have white spaces: 32407\n",
            "Removing Html tags....\n",
            "Number of rows have Html tags: 0\n",
            "Removing Punctuation....\n",
            "Number of rows have punctuation: 0\n",
            "Removing brackets....\n",
            "Number of rows have brackets: 0\n",
            "Number of rows have emojis: 55\n",
            "Number of non meaning  rows: (148, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is already loaded\n",
        "X = df['clean_comment']\n",
        "y = df['category']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)"
      ],
      "metadata": {
        "id": "mqb8H5ONmG5i"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dagshub.init(repo_owner='MitVinay', repo_name='youtube_chrome', mlflow=True)\n",
        "mlflow.set_experiment(\"Exp1-Feature_eng\")\n",
        "\n",
        "# Start parent run\n",
        "with mlflow.start_run() as parent_run:\n",
        "# Define the n-grams range\n",
        "  n_grams = [(1, 1), (2, 2), (3, 3)]\n",
        "\n",
        "  # Automate the process for both CountVectorizer and TfidfVectorizer\n",
        "  vectorizers = {\n",
        "      \"CountVectorizer\": CountVectorizer,\n",
        "      \"TfidfVectorizer\": TfidfVectorizer\n",
        "  }\n",
        "\n",
        "  for ngram in n_grams:\n",
        "      for vect_name, vect_class in vectorizers.items():\n",
        "          with mlflow.start_run(nested=True, run_name=f\"{ngram} using {vect_name}\") as child_run:\n",
        "            print(f\"Testing with n-gram range: {ngram} using {vect_name}\")\n",
        "            # Initialize the vectorizer\n",
        "            vectorizer = vect_class(ngram_range=ngram, max_features=1000)\n",
        "            X_train_vect = vectorizer.fit_transform(X_train)\n",
        "            X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "            # Train the RandomForestClassifier\n",
        "            rf = RandomForestClassifier(random_state=42)\n",
        "            rf.fit(X_train_vect, y_train)\n",
        "\n",
        "            # Predict and evaluate\n",
        "            y_pred = rf.predict(X_test_vect)\n",
        "            metrics = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "            for label, metrics_dict in metrics.items():\n",
        "              if label != 'accuracy':  # 'accuracy' is logged separately as a single value\n",
        "                  for metric, value in metrics_dict.items():\n",
        "                      mlflow.log_metric(f\"{label}_{metric}\", value)\n",
        "              else:\n",
        "                  # Log the accuracy score separately\n",
        "                  mlflow.log_metric(\"accuracy\", metrics_dict)\n",
        "            mlflow.log_param(\"max_features\", 1000)\n",
        "            mlflow.log_param(\"ngram_range\", ngram)\n",
        "            mlflow.log_param(\"vectorizer\", vect_name)\n",
        "            mlflow.sklearn.log_model(rf, \"model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669,
          "referenced_widgets": [
            "eb47cdcdad974df1945a34f2b20d6504",
            "7451a201701a47ebb817a004a7fd469f"
          ]
        },
        "id": "elYOdiTfmFJp",
        "outputId": "53f60ad1-6492-482c-fd86-669caebd2c91"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1mтЭЧтЭЧтЭЧ AUTHORIZATION REQUIRED тЭЧтЭЧтЭЧ\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">тЭЧтЭЧтЭЧ AUTHORIZATION REQUIRED тЭЧтЭЧтЭЧ</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb47cdcdad974df1945a34f2b20d6504"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=ae87f971-2218-403d-9a78-a34d380265b6&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=4519d70ace4d7f4f33f287185eb46cfe5c3cf87e644437ca464ab8f9cac5166f\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as MitVinay\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as MitVinay\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"MitVinay/youtube_chrome\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"MitVinay/youtube_chrome\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository MitVinay/youtube_chrome initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository MitVinay/youtube_chrome initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/12/11 21:32:38 INFO mlflow.tracking.fluent: Experiment with name 'Exp1-Feature_eng' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with n-gram range: (1, 1) using CountVectorizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2024/12/11 21:33:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ЁЯПГ View run (1, 1) using CountVectorizer at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2/runs/ea687ad3f82e4a24ad19bdf5b9d7d909\n",
            "ЁЯзк View experiment at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2\n",
            "Testing with n-gram range: (1, 1) using TfidfVectorizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2024/12/11 21:35:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ЁЯПГ View run (1, 1) using TfidfVectorizer at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2/runs/b5c1eea96af4419aade230b34de93e89\n",
            "ЁЯзк View experiment at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2\n",
            "Testing with n-gram range: (2, 2) using CountVectorizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2024/12/11 21:36:03 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ЁЯПГ View run (2, 2) using CountVectorizer at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2/runs/df7e7b34815c450aae300ca068557414\n",
            "ЁЯзк View experiment at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2\n",
            "Testing with n-gram range: (2, 2) using TfidfVectorizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2024/12/11 21:36:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ЁЯПГ View run (2, 2) using TfidfVectorizer at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2/runs/15ff7a5e5e42455291943ba4bd0b1b73\n",
            "ЁЯзк View experiment at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2\n",
            "Testing with n-gram range: (3, 3) using CountVectorizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2024/12/11 21:37:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ЁЯПГ View run (3, 3) using CountVectorizer at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2/runs/b97079f5cb2c463eb99f54ae6785ce2f\n",
            "ЁЯзк View experiment at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2\n",
            "Testing with n-gram range: (3, 3) using TfidfVectorizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2024/12/11 21:37:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ЁЯПГ View run (3, 3) using TfidfVectorizer at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2/runs/a1c599d3ee774b4b92c1daa9c638b1f5\n",
            "ЁЯзк View experiment at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2\n",
            "ЁЯПГ View run hilarious-bass-638 at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2/runs/da7ec10d59914093a75e005ccd1d662a\n",
            "ЁЯзк View experiment at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d8eM04eFgOP9"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}
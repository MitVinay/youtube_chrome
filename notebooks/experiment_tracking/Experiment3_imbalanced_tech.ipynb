{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "468abad75a534f82a635beb5e8740791": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_b611542012c6471b9b1d1890cec02ffe",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m‚†º\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚†º</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "b611542012c6471b9b1d1890cec02ffe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ! pip install nltk textacy\n",
        "# ! pip install scikit-learn\n",
        "# ! pip install mlflow\n",
        "# ! pip install dagshub\n",
        "# ! pip install imbalanced-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z87hUO1WeP5Q",
        "outputId": "9ee6989b-2c9d-4518-fdc2-8700f486423c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubApLNiRdL8o",
        "outputId": "cccd711b-bcc1-421f-c9ca-84a16109ac7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from textacy import preprocessing\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import mlflow\n",
        "import dagshub\n",
        "from collections import Counter\n",
        "from imblearn.under_sampling import ClusterCentroids\n",
        "from imblearn.under_sampling import CondensedNearestNeighbour\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.under_sampling import OneSidedSelection\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processing():\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/Himanshu-1703/reddit-sentiment-analysis/refs/heads/main/data/reddit.csv')\n",
        "  print(\"Shape of the data frame\", df.shape)\n",
        "  print(\"Duplicates\", df.duplicated().sum())\n",
        "  print(\"Null Values:\" ,df.isnull().sum())\n",
        "\n",
        "  print(\"Dropping the duplicate records.....\")\n",
        "  df.drop_duplicates(inplace=True)\n",
        "\n",
        "  print(\"Dropping the null values\")\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  print(\"Changing data to lower case\")\n",
        "  df['clean_comment'] = df['clean_comment'].str.lower()\n",
        "\n",
        "  df['length_clean_comment'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "\n",
        "  print(\"Strip off the white spaces..\")\n",
        "  df['clean_comment'] = df['clean_comment'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "  df['length_clean_comment_nowhite_space'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have white spaces:\" , df[df['length_clean_comment']!= df['length_clean_comment_nowhite_space']].shape[0])\n",
        "\n",
        "  print(\"Removing Html tags....\")\n",
        "  df['clean_comment'] = df['clean_comment'].apply(preprocessing.remove.html_tags)\n",
        "  df['length_nowhite_space_htmltag'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have Html tags:\" , df[df['length_clean_comment_nowhite_space']!= df['length_nowhite_space_htmltag']].shape[0])\n",
        "\n",
        "  print(\"Removing Punctuation....\")\n",
        "  df['clean_comment'] = df['clean_comment'].apply(preprocessing.remove.punctuation)\n",
        "  df['length_htmltag_punctuation'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have punctuation:\" , df[df['length_nowhite_space_htmltag']!= df['length_htmltag_punctuation']].shape[0])\n",
        "\n",
        "  print(\"Removing brackets....\")\n",
        "  df['clean_comment'] = df['clean_comment'].apply(preprocessing.remove.punctuation)\n",
        "  df['length_punctuation_brackets'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have brackets:\" , df[df['length_htmltag_punctuation']!= df['length_punctuation_brackets']].shape[0])\n",
        "\n",
        "  # Apply the function to the 'clean_comment' column in a single line\n",
        "  df['clean_comment'] = df['clean_comment'].apply(lambda x: preprocessing.replace.emojis(x, \"\"))\n",
        "  df['length_brackets_emojis'] = df['clean_comment'].apply(lambda x: len(str(x)))\n",
        "  print(\"Number of rows have emojis:\" , df[df['length_punctuation_brackets']!= df['length_brackets_emojis']].shape[0])\n",
        "\n",
        "\n",
        "  # Regular expression to match emojis\n",
        "  emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\"  # Emoticons\n",
        "                            \"\\U0001F300-\\U0001F5FF\"  # Symbols and Pictographs\n",
        "                            \"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
        "                            \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
        "                            \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                            \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                            \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                            \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                            \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                            \"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "                            \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
        "                            \"]\", flags=re.UNICODE)\n",
        "\n",
        "  # Filter out rows where 'clean_comment' contains emojis\n",
        "  print(\"Number of non meaning  rows:\", df[df['clean_comment'].apply(lambda x: bool(emoji_pattern.search(x)))].shape)\n",
        "  df = df[~df['clean_comment'].apply(lambda x: bool(emoji_pattern.search(x)))]\n",
        "\n",
        "\n",
        "  # List of words to keep even if their length is less than 4\n",
        "  keep_words = ['lol', 'wow', 'wtf', 'fun', 'sad', 'old']\n",
        "\n",
        "  # Filter out rows with clean_comment length < 4 unless they contain one of the keep_words\n",
        "  df = df[(df['length_clean_comment'] >= 4) | df['clean_comment'].isin(keep_words)]\n",
        "\n",
        "  return df[['clean_comment', 'category', 'length_clean_comment']]\n",
        "\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_comment(comment):\n",
        "    # Convert to lowercase\n",
        "    comment = comment.lower()\n",
        "\n",
        "    # Remove trailing and leading whitespaces\n",
        "    comment = comment.strip()\n",
        "\n",
        "    # Remove newline characters\n",
        "    comment = re.sub(r'\\n', ' ', comment)\n",
        "\n",
        "    # Remove non-alphanumeric characters, except punctuation\n",
        "    comment = re.sub(r'[^A-Za-z0-9\\s!?.,]', '', comment)\n",
        "\n",
        "    # Remove stopwords but retain important ones for sentiment analysis\n",
        "    stop_words = set(stopwords.words('english')) - {'not', 'but', 'however', 'no', 'yet'}\n",
        "    comment = ' '.join([word for word in comment.split() if word not in stop_words])\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    comment = ' '.join([lemmatizer.lemmatize(word) for word in comment.split()])\n",
        "\n",
        "    return comment"
      ],
      "metadata": {
        "id": "7NHAPG0VeVC6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pre_processing()\n",
        "df['clean_comment'] = df['clean_comment'].apply(preprocess_comment)\n",
        "# Assuming df is already loaded\n",
        "X = df['clean_comment']\n",
        "y = df['category']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "# Initialize the vectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,1), max_features=5000)\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LApIIS45ebdf",
        "outputId": "7f2d45f8-e11d-4865-d044-0401a0fcb131"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the data frame (37249, 2)\n",
            "Duplicates 449\n",
            "Null Values: clean_comment    100\n",
            "category           0\n",
            "dtype: int64\n",
            "Dropping the duplicate records.....\n",
            "Dropping the null values\n",
            "Changing data to lower case\n",
            "Strip off the white spaces..\n",
            "Number of rows have white spaces: 32407\n",
            "Removing Html tags....\n",
            "Number of rows have Html tags: 0\n",
            "Removing Punctuation....\n",
            "Number of rows have punctuation: 0\n",
            "Removing brackets....\n",
            "Number of rows have brackets: 0\n",
            "Number of rows have emojis: 55\n",
            "Number of non meaning  rows: (148, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imabalance Learnig\n",
        "1. UnderSampling\n",
        "2. OverSampling\n",
        "3. SMote ENN\n",
        "4. ADASYN\n",
        "5 Class Weight"
      ],
      "metadata": {
        "id": "-zEklxz_ejFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted(Counter(y_train).items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeszzRjsh7aL",
        "outputId": "ee4d4ad4-a6c8-44cd-ece8-48df1231f830"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(-1, 5759), (0, 8858), (1, 11014)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cc = ClusterCentroids(random_state=0)\n",
        "X_resampled, y_resampled = cc.fit_resample(X_train, y_train)\n",
        "print(sorted(Counter(y_resampled).items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWwfro4XfYmI",
        "outputId": "aca8f9c5-89b3-4163-c0d1-4fba2910541c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(-1, 5759), (0, 5759), (1, 5759)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = rf.predict(X_test)\n",
        "metrics = classification_report(y_test, y_pred, output_dict=True)\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rxmQiwUjcA0",
        "outputId": "716cf676-583e-4383-d99e-86568475f31d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'-1': {'precision': 0.7205580631924497, 'recall': 0.7069243156199678, 'f1-score': 0.7136760820971347, 'support': 2484.0}, '0': {'precision': 0.7908825868009541, 'recall': 0.7904635761589404, 'f1-score': 0.7906730259671436, 'support': 3775.0}, '1': {'precision': 0.7690052356020942, 'recall': 0.7769784172661871, 'f1-score': 0.7729712661825071, 'support': 4726.0}, 'accuracy': 0.765771506599909, 'macro avg': {'precision': 0.7601486285318327, 'recall': 0.7581221030150318, 'f1-score': 0.7591067914155952, 'support': 10985.0}, 'weighted avg': {'precision': 0.7655682055165357, 'recall': 0.765771506599909, 'f1-score': 0.7656462689971578, 'support': 10985.0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "undersampling_alo = {\"ClusterCentroids\": ClusterCentroids(random_state=0),\n",
        "                     \"CondensedNearestNeighbour\": CondensedNearestNeighbour(random_state=0),\n",
        "                     \"TomekLinks\": TomekLinks(),\n",
        "                     \"RandomUnderSampler\": RandomUnderSampler(random_state=0),\n",
        "                     \"OneSidedSelection\": OneSidedSelection(random_state=0),\n",
        "                     \"CondensedNearestNeighbour\": CondensedNearestNeighbour(random_state=0)\n",
        "                     }\n"
      ],
      "metadata": {
        "id": "PZOfgcuOlNWl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dagshub.init(repo_owner='MitVinay', repo_name='youtube_chrome', mlflow=True)\n",
        "mlflow.set_experiment(\"Exp3-Undersampling Technique\")\n",
        "\n",
        "# Start parent run\n",
        "with mlflow.start_run() as parent_run:\n",
        "    # Define the n-grams range and max_features values\n",
        "\n",
        "    # Automate the process for TfidfVectorizer\n",
        "    for undersample, algo in undersampling_alo.items():\n",
        "        with mlflow.start_run(nested=True, run_name=f\"{undersample} TFIDF , 1gram max_features=5000\") as child_run:\n",
        "            print(f\"undersample: {undersample}\")\n",
        "\n",
        "            X_resampled, y_resampled = algo.fit_resample(X_train, y_train)\n",
        "            print(sorted(Counter(y_resampled).items()))\n",
        "            # Train the RandomForestClassifier\n",
        "            rf = RandomForestClassifier(random_state=42)\n",
        "            rf.fit(X_resampled, y_resampled)\n",
        "\n",
        "            # Predict and evaluate\n",
        "            y_pred = rf.predict(X_test)\n",
        "            metrics = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "            for label, metrics_dict in metrics.items():\n",
        "                if label != 'accuracy':  # 'accuracy' is logged separately as a single value\n",
        "                    for metric, value in metrics_dict.items():\n",
        "                        mlflow.log_metric(f\"{label}_{metric}\", value)\n",
        "                else:\n",
        "                    # Log the accuracy score separately\n",
        "                    mlflow.log_metric(\"accuracy\", metrics_dict)\n",
        "\n",
        "            mlflow.log_param(\"max_features\", 5000)\n",
        "            mlflow.log_param(\"ngram_range\", 1)\n",
        "            mlflow.log_param(\"vectorizer\", \"TfidfVectorizer\")\n",
        "            mlflow.log_param(\"Under Sampling Technique\", algo)\n",
        "            mlflow.sklearn.log_model(rf, \"model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "468abad75a534f82a635beb5e8740791",
            "b611542012c6471b9b1d1890cec02ffe"
          ]
        },
        "id": "JJpRTTBWpxkR",
        "outputId": "03c2dbe3-b80b-4f10-bafb-ce52b4d613e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1m‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "468abad75a534f82a635beb5e8740791"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=8af0883c-9a71-4c8d-834a-17f136a0710c&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=7e85f2f7f68e5be98a59af5344dfba88479d32f7540b6105ac0b2da3561c2373\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as MitVinay\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as MitVinay\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"MitVinay/youtube_chrome\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"MitVinay/youtube_chrome\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository MitVinay/youtube_chrome initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository MitVinay/youtube_chrome initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/12/13 15:41:22 INFO mlflow.tracking.fluent: Experiment with name 'Exp3-Undersampling Technique' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "undersample: ClusterCentroids\n",
            "[(-1, 5759), (0, 5759), (1, 5759)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2024/12/13 15:51:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèÉ View run ClusterCentroids TFIDF , 1gram max_features=5000 at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/4/runs/62ae70686fb84374b55bd4a9145a0279\n",
            "üß™ View experiment at: https://dagshub.com/MitVinay/youtube_chrome.mlflow/#/experiments/4\n",
            "undersample: CondensedNearestNeighbour\n"
          ]
        }
      ]
    }
  ]
}